"ReadingQizMaker: 一个人工智能与自然语言处理协作系统，支持教师设计高质量的阅读测验问题。"

图1：ReadingQuizMaker界面包含导航栏（a）、论文面板（中间）和问题创作面板（右侧）。用户可以使用导航栏导航论文。绿色块表示表格和图形。用户可以在论文面板中选择内容，将其传输到问题创作面板以创建问题。ReadingQuizMaker根据用户对文本的选择提供人工智能建议（例如，改述）（c）。用户的选择被突出显示（b）。在问题创作面板上，用户可以编辑问题主干和选项（f），并预览更多的人工智能建议（g）。用户可以在基于部分的模式和普通状态之间切换（d），并查看他们创建的所有问题（e）。

摘要
尽管阅读任务普遍存在，但鼓励学生积极阅读的方法有限。我们提出了一个系统ReadingQuizMaker，支持教师方便地设计高质量问题，帮助学生理解阅读材料。ReadingQuizMaker适应教师自然的问题创作工作流程，同时提供基于自然语言处理的过程导向支持。ReadingQuizMaker使教师能够决定何时以及使用哪些自然语言处理模型，选择输入模型的内容，并编辑结果。在评估研究中，教师发现生成的问题与他们之前设计的测验相媲美。教师赞扬ReadingQuizMaker易于使用，并认为自然语言处理的建议令人满意且有帮助。我们将ReadingQuizMaker与一个对照条件进行比较，在该条件下，教师得到自动生成的问题进行编辑。教师明显更倾向于由ReadingQuizMaker提供的人工智能与人类协作的方法。我们的研究结果表明，在提供人工智能支持时，给予用户控制权并立即展示人工智能结果的预览至关重要。

1 引言
指定阅读是几乎所有大学课程的一部分。教师认为课堂阅读是重要的学习活动，可以增强课堂讨论[17, 46]。然而，研究表明，学生未完成阅读任务已成为高等教育中的一个普遍问题[13, 15, 18, 24, 26]。在不同学科领域的多项研究中，估计只有20-30%的本科生阅读了为课程分配的材料[13, 15, 26]。随着社交媒体的普及和信息呈现方式的短平快，过去十年大学阅读进一步下降[29, 76]。低合规性的原因多种多样，可能是学生缺乏动力[15, 51]，阅读技能不足[72]，时间受限[63, 67]，以及低估阅读的重要性[43, 62]。

教师如何更好地支持学生的学术阅读实践？教学策略和数字工具已被提出以协助学生的阅读体验。近年来，许多教师选择使用社交标注工具。例如，Perusall [68] 就是这类流行工具之一，广泛用于课前阅读任务。研究表明，使用Perusall后学生花费更多时间阅读，并在课堂考试中表现更好[60]。然而，研究还表明，社交标注工具仅对具有自我调节学习技能的学生更有效[22]。这类工具的一个弱点在于学生无法获得关于他们对内容理解的反馈[9]。多年来，阅读问题是教师积极引导学生参与的常见策略。一方面，与被动阅读文本相比，问题回答提供了一种积极的学习体验[23, 34]，通过几十年的教育研究证明对提高理解和学习成果是有效的[9, 23, 48, 77]。另一方面，学生可以得到即时反馈[77]，帮助他们自我评估对文本的理解或者回头仔细阅读某些部分[46]。对于在学术阅读方面不太熟练的学生，精心设计的阅读问题有助于他们集中注意力并提取关键信息[28, 72]，否则这些信息可能会丢失[72]。

然而，设计高质量且引人思考的问题需要大量的努力。由于这些问题只是检查学生是否阅读了特定内容[71, 72]，因此教师和学生都不喜欢为阅读设计以细节为导向的测验问题。正如先前的工作建议的，好的阅读问题应该在学生阅读时引导他们，帮助他们识别重要内容，强调到最后他们应该理解的内容，引发对内容中主要问题和含义的思考，并使学生准备好在课堂上讨论阅读材料[32]。尽管存在许多基于自然语言处理的自动问题生成系统，但它们在课堂中的应用较少[8, 49]，主要是因为这些模型仅适用于特定领域，如语言学习和数学教学，生成的问题通常质量低且类型和难度水平有限[8, 49]。

在这项工作中，我们提出了一个人工智能与自然语言处理协作的系统ReadingQuizMaker，以支持教师创建高质量的阅读问题。ReadingQuizMaker适应于教师自然的问题构建工作流程，同时提供基于自然语言处理的过程导向支持。ReadingQuizMaker的设计基于对来自7所不同大学的11名教师的需求研究，该研究表明，由于在创建问题时依赖领域专业知识和外部资源，教师输入在问题创作过程中至关重要。在提供问题创作的人工智能协助时，教师希望确保他们在何时以及如何使用人工智能方面具有完全的控制和灵活性[10]。

ReadingQuizMaker的概览如图1所示。用户可以在论文面板上选择文本，并将其发送到问题创作面板作为问题选项。ReadingQuizMaker使用用户选择的文本提供实体替换和改述等人工智能建议的即时预览。在问题创作面板上，用户会得到有关问题主干的建议，并可以使用NLP工具箱改进问题选项并创建由否定模型支持的干扰项。教师使用过的所有文本都在论文面板上突出显示，并通过导航栏可视化，允许用户检查内容覆盖范围。

我们进行了一项评估研究，共有来自10所不同大学的13名教师参与，以评估ReadingQuizMaker。所有参与者都成功地使用ReadingQuizMaker创建了他们满意的问题。参与者评论说界面易于使用，帮助他们创建更好的问题，并相对于他们通常的教学设计实践来说能够节省时间。参与者还发现ReadingQuizMaker中的人工智能建议是有用且令人满意的。大约60%的人工智能改述建议在用户阅读后被采纳。大约60%的人工智能否定建议被用户采纳。与总结建议相比，用户更常阅读和采纳改述建议，因为改述建议更容易被发现，而无需用户明确请求。用户分享说他们从人工智能中得到了灵感，并确保在使用之前阅读人工智能的结果以避免错误。

在研究中，我们将ReadingQuizMaker与一个基准条件进行了比较，其中教师收到了自动生成的问题并有机会对其进行编辑和改进。参与者强烈偏好ReadingQuizMaker提供的人工智能与人类协作的方法，并发现自动生成的问题质量较低。教师还发现与从头开始创建问题相比，编辑自动生成的问题更具挑战性。参与者认为在问题创作过程中具有控制感是至关重要的。

本文提出了以下三个贡献：
• 通过一个形成性研究揭示了教师在创建阅读测验问题时面临的挑战，以及为开发支持该过程的工具制定的设计要求。
• ReadingQuizMaker，这是一个新颖的系统，为教师在创建问题时提供基于自然语言处理的过程导向支持。ReadingQuizMaker的设计适应于教师自然的问题创作工作流程，通过新颖的交互设计加速问题创作过程，并为教师提供人工智能建议以增强他们的问题创作体验。
• 对ReadingQuizMaker的评估展示了系统的可用性和实用性。该研究显示了人工智能与人类协作方法在支持创造性教学设计工作方面的潜力，并建议在提供人工智能支持时给予用户控制，并显示人工智能结果的即时预览以增加采用率的重要性。

2 相关工作
2.1 大学阅读任务的低合规性
大学生对阅读任务的合规性较低。研究发现，只有20% - 30%的本科生会为课程进行阅读[13]，这导致了不理想的学业表现[24, 73]。低合规性的一个主要原因是缺乏动力[17, 50, 51]。许多学生低估了阅读的重要性[73]。研究发现，学生将阅读视为对讲座的补充[16]，并倾向于不花额外的时间进行阅读[13, 16, 62]。阅读技能的不足也导致学生对阅读的低合规性[17, 72]，尤其是当他们遇到阅读任务和可视化日益复杂时[72]。时间限制被排名为阻止学生完成阅读任务的“首要约束”[13, 46]。这表明需要更好的实践来帮助学生阅读，通过降低阅读任务的难度，为阅读技能不足的学生提供支持和练习，并为学生的阅读过程提供反馈以提高参与度。

2.2 支持阅读实践的策略
先前的研究提出了许多策略来帮助学生积极阅读。一条研究线路鼓励学生采取积极的笔记，例如学习日志[20]或索引卡[19]。其他工作支持协作阅读[44]，例如使学生分享笔记[59]，制作播客[12]，在讨论论坛或社交媒体上分享帖子[29, 44]。社交标注工具如Perusall [68] 被发现在增加学生阅读时间方面是有效的。这些策略强调了积极阅读的重要性，其中活动旨在在阅读过程中保持学生的积极注意力。然而，研究还表明，许多这些策略仅对具有自我调节学习技能的学生更有效[22]。这类工具的一个弱点在于学生无法获得关于他们对内容理解的反馈[9]。另一种经常使用的方法是阅读测验问题[13, 35]，采用多项选择题的形式，以实现即时反馈。然而，设计高质量且引人思考的阅读测验问题可能会很困难[81, 82]。

2.3 支持积极阅读的界面
一条研究线路专注于在数字环境中模仿纸质阅读体验[64]，重点关注导航和笔记。例如，Pearson等人开发了可以用作数字阅读书签的数字贴纸[66]。LiquidText引入了一个工作区，供用户与他们的评论互动[80]。更近期的工作包括采用新颖的UI设计来增强科学论文阅读体验[36, 37]，支持理解公式符号和数学。在我们的工作中，我们专注于开发技术来支持高等教育背景下的阅读理解。我们的目标是支持教师创建高质量的阅读问题，进而帮助学生对文本的概念理解。

2.4 用于教育目的的问题生成技术
随着对积极阅读和积极学习重要性认识的增强，研究人员在人工智能和教育交叉领域开发了支持问题创作的技术。一条研究线路使用众包技术生成新问题。例如，UpGrade根据先前学生的解决方案创建问题[83]，而QMAps鼓励学生为彼此生成问题[91]。另一条研究线路开发了端到端的自然语言处理模型用于问题创作。现有的自动问题生成技术擅长创建事实性问题[27, 49]，但无法生成针对更高的布鲁姆目标的问题[14]。在问题主干和开放式问题生成方面，Willis等人使用KPE-Gen提取关键短语并生成问题主干。BERT [78]和PLM [85]模型也用于创建开放式问题。QG-Net训练了一个循环神经网络结构，以整合上下文信息并逐字生成wh格式的问题[84]。在多项选择问题生成方面，先前的方法使用命名实体识别和主题建模来识别显著的句子并提取问题选项的关键词[54, 55]。然而，现有问题生成系统的主要缺点是它们通常只适用于单一领域[79, 92]，生成的问题通常质量低且类型和难度水平有限[21, 39, 49, 65]。在这项工作中，我们引入了一种人工智能与人类协作的方法，其中在设计多项选择问题时，人工智能为人类教师提供过程导向支持。目标是使问题创作过程更为稳健和灵活，并产生更高质量的问题。

2.5 用于教育的人工智能系统
由于纯人工智能系统往往具有模型能力的高度不确定性和模型输出的高度复杂性[89]，人工智能与人类的协作在各种领域得到了探索[25, 86]。这个概念近年来在教育领域被引入并进行研究。人工智能教育系统通常由人类教师引导教学决策过程，而人工智能在整个过程中提供支持。先前的研究探讨了各种各样的人工智能方法，主要集中在支持课堂教学，例如通过仪表板向教师可视化学生的进展和困扰[38, 57, 58]，智能可穿戴设备[42, 70]和环境感知工具[7]，帮助教师分配学生到团队[87]，以及改善课堂协同[56, 88]。人工智能在教学和教学设计的其他阶段的支持效果尚未得到深入探讨，例如帮助教师准备材料和问题[74]。在这项工作中，我们研究了人工智能与人类协作的方法在支持教师创建测验问题方面的能力。

2.6 人工智能系统设计准则
近年来，研究人员提出了用于设计和开发人工智能系统的准则[10]。在其中一个被广泛采纳的准则中，Amershi等人提出了18个适用于不同交互场景的准则，以改善用户体验。与这项工作特别相关的一些准则包括“显示相关上下文信息”、“支持高效调用”、“支持高效解雇”和“支持高效更正”[10]。ReadingQuizMaker的设计受到这些原则的启发，我们的目标是检查用户在一个依赖主题专业知识的创造性和高风险任务上如何与人工智能系统互动。此外，Holstein等人提出，在设计人工智能系统时，需要在所有阶段涉及从业者并进行迭代改进[41]。Yang等人[89]表明用户更喜欢高召回率系统而不是高精确度系统[47]，这表明用户需要控制何时以及如何使用人工智能的结果。ReadingQuizMaker的设计是通过与11名大学教授进行深入的形成性研究，并经过多轮试验测试，以确保人工智能与人类的互动对用户来说是直观且可取的。

3 形成性调查
我们对11位大学教师进行了一项形成性研究，以了解他们创建问题的自然工作流程，目的是理解教师手写问题时面临的独特挑战，并总结开发支持该过程的以用户为中心的系统的设计要求。

3.1 参与者和程序
这项形成性研究已经获得IRB批准。来自7所不同大学的11位教师参与了研究（男性6位，女性5位）。这些教师的教学经验范围从2年到40年不等，涉及计算机科学、信息科学、数据科学、教育、发展心理学和政治科学等学科。
采访通过Zoom进行，每次持续50到75分钟。参与者获得了50美元的礼品卡。我们首先要求参与者分享他们如何处理阅读作业。大部分时间都花在让参与者根据他们选择的阅读文本设计问题上。在整个过程中没有提供支持。具体来说，我们要求他们设计问题（最好是多项选择题），以帮助他们的学生理解和从内容中学到东西。我们要求参与者在整个过程中大声思考。
在会话期间，参与者能够设计3到10个多项选择题（一个问题干和四个选项）。然后，我们要求参与者反思他们是如何得出每个问题干和选项的，并分享他们在整个过程中遇到的挑战。最后，研究人员要求参与者想象有一个智能系统可以在测验设计过程中提供支持。具体而言，研究人员询问用户对一系列在其问题创建过程中情境化的自然语言处理任务的态度，例如，“如果系统可以为您改写这个句子，您会怎么想？”我们将采访记录进行了转录，并使用亲和图[61]分析了我们的数据。

3.2 结果
3.2.1 设计高质量的阅读问题是令人向往但难以实现的。所有参与者都提到应该有更好的方法来支持学生阅读。例如，P5说：“这绝对是教师面临的问题。”参与者分享了他们用来支持阅读的技巧以及这些方法的局限性。例如，阅读摘要在评分方面存在挑战，协作标注平台（如Perusall [68]）鼓励参与，但不一定确保学生获得关键信息。P7赞赏Perusall的许多功能，同时提出了一个担忧，“类似的协作标注工具可能将阅读视为机械化，并衡量他们是否阅读，而不是是否找到了洞察力。”一些参与者已经在使用测验问题来提升阅读。大多数参与者表示，如果问题设计变得更便宜和更容易获得，他们对使用测验问题很感兴趣。

3.2.2 我想使用阅读问题引导学生思考。
尽管一些教师喜欢使用阅读的测验问题，但他们也强调问题应该以正确的方式设计。1）教师希望询问综合知识，而不是要求学生进行模式匹配的问题。P2提到他对提出“为什么”问题很感兴趣，例如，“为什么这么难”。P1提到批判性思维很重要，“但这篇论文本身有很多深层次的缺陷。我希望学生能看到这些缺陷。” P5提到他们鼓励学生跳出材料进行思考。

3.2.3 我在编写测验题时面临挑战。参与者在编写测验题的过程中报告了各种挑战。几乎所有参与者表示这对他们来说是一项耗时的任务，他们可能会花费相当多的时间在此上。最突出的两个挑战是：1）识别问题机会；2）构思干扰项。大多数参与者首先浏览文本，以识别他们认为对学生了解重要的内容，然后为其设计问题。例如，P4说：“我首先需要思考文本中的关键概念是什么”。几乎所有参与者表示，构思干扰项很困难。教师希望干扰项引人深思（P7），具有说服力（P6），并揭示学生的误解，提出课堂上讨论概念的机会（P5）。P5表示他们经常不确定他们构思的干扰项是否好，而且认为开放式问题更有助于帮助教师引出学生的误解。
参与者提到的其他挑战包括构思问题干、获得全面的内容覆盖和准确的内容细节。教师发现编写问题干很难，因为它们应该根据论文类型和学习成果进行定制。多位参与者提到他们希望相对全面地覆盖内容，但很难在不同点上衡量覆盖范围。此外，教师希望在问题选项中准确无误，并确保正确答案完全正确，错误答案是错误的。

3.2.4 如果它们是好的、可控制的和透明的，NLP支持可能会很有用。参与者通常对使用自然语言处理（NLP）工具来支持他们的过程的想法作出积极回应。与此同时，参与者表示希望在过程中有控制权，以决定是否使用NLP的结果。教师对何时使用不同的NLP模型持有不同的意见。例如，P6说：“我认为改写可能会有帮助，因为我认为那对我来说是最困难的部分。” 他还认为总结对于他缩短复读时间会有帮助，“如果我能够让[文本的一部分]被总结，如果我信任智能，它就会像这样，这是这个的要点。所以它只是节省时间。”参与者提到他们对支持的使用将取决于模型的性能。他们认为在某些情境中否定可能是有帮助的，特别是如果他们（用户）能够提供关键字，“这将使它更准确，更符合我的需求”。P7强调他们希望智能系统在提供建议的同时能够保持控制。

3.3 设计要求
根据需求调查研究，我们总结了开发以教师为中心的测验设计系统的以下设计要求。设计要求也与先前关于人机交互设计准则的文献[10, 90]相对应。
• 支持教师创建有说服力的干扰项。许多参与者表示在生成干扰项方面有困难，P4说：“我希望[干扰项]对于他们来说不容易猜到，但我也不希望它太棘手”。大多数参与者表示，他们希望在创建有意义的干扰项方面得到支持。
• 提供过程导向的支持，使教师能够融入他们的专业知识。教师不喜欢传统的端到端AI方法来创建问题。教师希望在需要时灵活决策并融入他们的专业知识，例如课程的背景、学生的背景知识等。
• 问题创建需要快速且与教师当前的工作流程整合。大多数参与者希望缩短他们在问题创建上的时间。例如，P7和P9提到，他们希望尽可能少地花时间编写问题，而P10建议有一个系统来处理较低层次的问题将有助于他们的工作流程。
• 使教师能够轻松为问题编写反馈。反馈对教师非常重要，P5讨论了他们如何在注意到学生存在误解时“通常会给予学生一些建议”。参与者表示，为问题提供反馈的机制，尤其是对于不正确的答案，是至关重要的。
• 在与AI交互时给教师一种控制感 当被问及是否愿意接受AI支持时，教师表示他们希望保持控制（P6）。 P7评论说，如果AI提供建议，那将是有效的，但他们更愿意保持控制并在创建问题时嵌入自己的知识。这符合人机交互准则[10]中的多个原则，以支持高效的调用、解雇和更正。
• 给予教师在决策方面的灵活性 我们观察到不同的教师采用了不同的创建问题策略。教师对不同的问题类型有偏好。例如，他们可能会在大班上使用多项选择问题，在研讨会风格的研究生课程中使用开放式问题。教师还希望问题库具有不同难度水平。

4 READINGQUIZMAKER
根据形成性研究中确定的用户挑战和设计要求，我们开发了ReadingQuizMaker，这是一个为教师提供过程导向支持的系统，以创建符合其教育目标的高质量阅读测验问题。ReadingQuizMaker适应教师自然的测验设计过程。ReadingQuizMaker将阅读文本和问题创建面板并排放置，旨在缩短用户阅读文本的时间，如图1所示。我们首先描述一次用户使用系统的过程。然后，我们将根据上述设计要求描述与每个系统组件相对应的部分。

4.1 一个示例用户过程
Alice在大学任教，并将一篇关于增强现实（AR）主题的论文作为必读材料分配给她的班级。她希望创建阅读测验问题，引导学生阅读并帮助他们理解论文的主要观点。Alice使用ReadingQuizMaker创建问题。她将论文的HTML文件加载到系统中，并开始创建问题。

当Alice阅读摘要时，她得到了一个问题的想法。她希望学生知道这篇论文做了什么，作者发现了什么。她从菜单中选择一个问题干扰项：“以下关于论文的描述哪个是不正确的？”然后，Alice开始在Paper Panel中查找选项。在选择了一个句子后，ReadingQuizMaker会提出重新表述该句子的建议。Alice喜欢重新表述的结果。例如，原句“我们讨论学习和协作的差异，以及实施增强现实进行非结构化学习活动的好处和弊端”被重新表述为“作者讨论了增强现实对学习的好处和弊端”。在Alice成功创建了几个正确选项后，她需要想出一个干扰项。Alice打开Question Authoring Panel中的Transform Menu，并尝试对一个选项进行否定。Alice认为否定的结果还不错，稍作修改。Alice现在完成了第一个问题。随着Alice有更多的问题，她想检查内容覆盖范围。Alice查看Navigation Bar，了解哪些部分需要更多关注。然后，Alice回顾了她创建的所有问题并下载了它们。Alice现在可以将问题导入Canvas，并将测验用作课前阅读任务。

4.2 详细设计

4.2.1 与任何具有HTML源的文章兼容。ReadingQuizMaker将阅读来源限制为HTML文件。自2018年以来，ACM数字图书馆中的大多数学术出版物（例如ACM CHI，UIST，ICER）都有在线HTML版本，遵循ACM发布系统（TAPS）以增强可访问性[6，33]。其他出版商也正在支持学术出版物的在线HTML版本，包括Springer[3]和Taylor＆Francis Online[4]。ReadingQuizMaker还可以很好地与经常在编程课程中使用的在线文档和教程、在线教材[11]以及华盛顿邮报和Vox等新闻平台的文章[5]配合使用。
ReadingQuizMaker（RQM）使用iframe来显示文章的HTML文件，并保留原始格式。它移除HTML源文件中的所有JavaScript代码，以避免与系统功能产生冲突。对于ACM DL中的学术出版物，我们通过HTML标签解析了论文的结构，并提取了章节标题（例如，摘要、引言、相关工作等），然后在Qestion Authoring Panel中显示这些章节标题。对于来自其他来源的内容，在评估研究中，第一作者手动提取了章节标题。但是，用户可以选择使用纯文本模式，其中不显示章节标题。这使ReadingQuizMaker与任何具有HTML版本的文章兼容。

4.2.2 导航栏支持用户对内容覆盖范围的阅读和导航到图表和表格。在形成性研究中，许多用户明确提到他们希望学生阅读表格和图表。导航栏用绿色块突出显示阅读文本中的表格和图表，块的大小与图的大小成比例。这是使用pagemap实现的，它是一个用于minimap的npx包[45]。表格和图表是在将文件加载到界面时基于HTML标签自动提取的。此外，导航栏还显示用户高亮显示，指示问题中使用了阅读中的哪些内容。用户可以查看导航栏，了解阅读的哪个部分需要更多关注。

4.2.3 提供NLP建议的即时预览。ReadingQuizMaker根据用户的文本选择向用户提供AI建议。具体来说，用户在Paper Panel中阅读内容。当他们看到他们想要在问题中查看的句子时，他们可以选择文本并将其转移到Qestion Authoring Panel。系统提供两种选择文本的方式：1）拖动鼠标以进行精确选择；2）在句子内双击以选择整个句子。用户一旦选择了一些文本，系统会基于用户的选择立即预览AI建议，如图2所示。用户无需采取任何明确的操作即可查看AI建议。具体而言，系统提供了三种基于NLP的转换。当句子包含第一人称代词（例如“我们”或“我们的设计”）时，使用正则表达式将其替换为“作者”或“设计”。对于较短的句子，系统提供了一个释义建议。对于较长的文本或段落，系统提供简洁的摘要。如果用户喜欢使用AI建议，他们可以将其转移到Qestion Authoring Panel。用户也可以选择发送原始文本，具有这种灵活性。当用户发送文本以成为选项时，更多的AI建议将自动出现在添加的选项下面。用户可以决定是否使用它们。

4.2.4 适应用户的自然工作流程并使用户能够为选项编写反馈。Qestion Authoring Panel的设计旨在与用户创建问题的自然流程保持一致。该面板的概述如图3所示。系统支持三种类型的问题：多项选择题、多项响应题和开放性问题，如图3所示。对于所有问题类型，用户可以从Paper Panel中传递文本，从Paper Panel中传递图像，或通过自己添加内容（图1）自由添加内容。根据形成性研究，我们还确保用户可以轻松地为问题选项添加反馈，并通过向Notes字段添加注释使用户能够进行高层次的规划。
形成性研究中的许多参与者提到，对于他们来说，提出问题的干扰项是困难的。为了解决这个问题，ReadingQuizMaker提供了一个问题干扰项库，该库是从形成性研究中众包得到的，如图4所示。问题干扰项库包含28个问题模板。用户可以从干扰项库中选择，或者他们可以键入一个新的干扰项，系统会根据关键词匹配提供建议。系统根据用户所在的部分（例如，引言、相关工作）、问题类型（MCQ、MRQ、OEQ）以及用户是否选择了一个图表，提供不同的干扰项建议。表1显示了干扰项的示例。通过界面，用户还可以通过添加或删除“NOT”单词来否定问题干扰项。

4.2.5 NLP工具箱。在形成性研究中，我们发现参与者在创建问题时会根据上下文选择使用不同的策略。为了支持多功能问题的创建，在ReadingQuizMaker中，我们提供了一个NLP工具箱，用户可以在其中获得进一步的AI辅助，如图5所示。该工具箱首先加载选项中的原始文本。用户可以选择应用释义、摘要、否定操作来转换文本。由于大多数参与者发现提出干扰项具有挑战性，因此我们引入了否定模型以提供建议。我们还实现了用户可控制的否定版本，用户可以决定在句子中否定哪个词，如图6所示。用户选择否定单词“greater”。当用户选择一个单词时，系统提取参数之前和之后的7个单词，并将这15个单词发送到否定模型。然后，系统用原始句子中的否定单词替换。
除了单个操作之外，用户还可以通过链接来组合多个操作。图6显示了链接两个操作的示例。释义的结果被发送作为否定模型的输入。用户可以随时加载原始文本，如果他们改变主意的话。

4.2.6 Review and Output. 在用户创建问题的过程中，他们可以查看所有已创建的问题，如图7所示。用户在拥有足够的问题之后，可以将这些问题下载到一个.csv文件中，该文件格式已准备好以便转换为.QTI包。在QTI包中的测验与大多数学习管理系统兼容，包括Canvas。用户可以轻松地将他们在ReadingQuizMaker中创建的问题导入Canvas中。 这也满足了形成性研究中揭示的设计需求，即教师希望有一种无缝的方法来节省他们的时间。

Figure 2: ReadingQuizMaker 在用户的文本选择基础上立即提供AI建议，包括 1) 实体替换，将第一人称代词替换为第三人称代词；2) 对单个句子进行释义（左）；3) 对段落或长文本进行摘要（右）。用户可以选择采纳任何AI建议作为问题选项。用户还可以选择将摘要结果拆分为多个选项。

Figure 3: ReadingQuizMaker 的问题创作面板是可折叠的。系统支持3种问题类型（a）。用户可以为高级规划添加注释（b），添加/删除选项（e, c），为每个选项添加/删除反馈（d），复制先前的选项（f），并保存（g）。

Figure 4: ReadingQuizMaker 帮助用户提出问题的问题干。系统基于关键词提供茎的建议（a）。系统根据用户所在的部分（例如，引言，相关工作），问题类型（MCQ，MRQ，OEQ）以及用户是否选择了图表等提供不同的建议（b，c）。

4.3 自然语言处理模型
在ReadingQuizMaker系统中，应用了一系列基于Transformer的自然语言处理（NLP）模型，包括（1）一种抽象摘要模型，用于压缩长段落；（2）一种释义模型，用于释义和简化句子；（3）一种否定模型，用于生成不正确的选项。以下是这些模型的详细信息。
• 抽象摘要：我们使用了在CNN-DailyMail上微调的BART[52]模型，以在给定段落的情况下压缩内容。我们使用了HuggingFace的bart-large-cnn检查点。
• 释义：我们使用了在PEGASUS[93]上预训练的释义模型，以在保持语义信息的同时改写句子。我们采用了HuggingFace发布的pegasus_paraphrase检查点。
• 否定：我们应用了在WikiFactCheck-English[75]上微调的基于BART的否定性声明生成模型。我们采用了作者在HuggingFace上发布的检查点。

4.4 系统迭代
我们进行了三轮的试点测试，以交互式地改进系统设计。以下是我们迭代的主要内容。

4.4.1 提高AI功能的可发现性。在试点研究中，我们意识到用户倾向于不点击按钮来应用NLP转换。然而，当我们鼓励用户评估NLP建议时，他们发现这些建议是令人满意的。为了提高AI功能的可发现性，我们引入了NLP建议的即时预览，如图2所示。这避免了用户额外点击以获取支持，与人工智能交互准则[10]相一致，以支持高效的调用、撤销和更正。

4.4.2 可视化NLP转换。我们发现参与者需要花费时间比较NLP建议和原始文本。因此，我们在实体替换和否定转换的操作之前和之后进行了可视化，如图2和图6所示。对于抽象摘要和释义操作，我们没有实施可视化，因为它们基于生成模型，而且差异通常比几个词更引人注目。我们将留待未来的工作来探索更好的可视化技术，以帮助最终用户阅读和使用NLP结果。

4.4.3 引入阈值以提高NLP性能。在试点研究中，我们发现当用户将摘要操作应用于相对较短的文本时，效果不佳。基于多次迭代，我们引入了一个400个字符的阈值来决定使用哪些模型为用户提供建议。如果用户选择的文本较长，系统将提供摘要建议；否则，系统将提供释义建议，如图2所示。在NLP工具箱中，当选项长度低于阈值时，摘要操作将被禁用。

4.5 实施
ReadingQuizMaker被实现为一个全栈Web应用程序，具有用于托管NLP模型的后端服务器。用户界面采用React.js [31]和Django [30]框架编写。Web应用程序连接到一个由Python实现的后端服务器，该服务器接受来自Web应用程序的API调用（例如，释义），应用NLP操作并返回结果。Web应用程序通过DigitalOcean [2]部署，而后端服务器则作为AWS EC2实例 [1]部署。

5 评估研究

我们进行了一项经过IRB批准的评估研究，旨在了解ReadingQuizMaker的可用性和实用性。我们还对比了用户对ReadingQuizMaker作为人工智能协作系统与自动方法的反应，关注以下研究问题：
RQ1： ReadingQuizMaker是否可用？教师能否使用ReadingQuizMaker创建令其满意的问题？
RQ2： 教师如何看待人工智能建议？它们的质量是否令人满意？是否分散注意力？教师是否认为人工智能建议是有用的还是令人不满的？
RQ3： 教师如何比较由ReadingQuizMaker提供的人工智能协作方法与自动问题生成方法？
RQ4： 用户面临什么挑战，以及为开发教育领域的人工智能协作系统有哪些设计启示？

5.1 参与者招募
我们通过社交媒体（包括教授邮件列表和社交群组）以及线下通讯招募参与者。有来自10所大学的13名大学教师（9名男性，4名女性）参与了该研究。所有参与者都曾教授或设计过需要阅读的大学课程，并设计过用于支持学生阅读的讨论或测验问题。他们平均有4-5年的大学课程教学经验，最长的为17年。他们来自教育、信息科学、计算机科学、技术传播、工程教育和政治科学等不同学科。研究会话通过Zoom进行，每次持续90-100分钟。参与者以50美元礼品卡作为报酬。在研究会话之前，我们要求参与者选择一篇阅读文本，他们将在会话中使用。唯一的要求是文本需要有在线HTML来源。在13名参与者中，有7人使用了ACM数字图书馆的学术出版物，4人使用了在线教材章节，1人使用了在线教程，1人使用了一家新闻与评论网站的新闻文章。

5.2 自动生成问题的基准条件

为了回答RQ3，即教师如何看待人工智能协作方法与自动生成方法相比，我们引入了一个基准条件。我们开发了一个流程，从用户选择的阅读中自动生成多项选择题。在ReadingQuizMaker的试点测试中，我们观察到用户可能从一个或相邻的段落中提取句子，将这些句子改写为正确选项，然后对一句进行否定以作为错误选项。我们在设计流程时遵循了这种模式。

首先，我们使用BeautifulSoup [69]解析HTML文件以提取段落。将段落作为输入，我们应用了一个抽取性摘要模型来提取显著的句子。我们使用了BertSumExt [53]，它采用基于预训练的BERT模型的文档级编码器。我们采用了在CNN-DailyMail数据集上训练的已发布检查点 [40]。我们使用这个模型为每个段落提取了两个显著的句子。然后，我们组合两个相邻的段落以生成一个问题的选项，使得每个问题涉及两个段落。问题干扰项根据模板自动生成，如“根据[章节标题]，以下哪一项是不正确的”。在前一步骤中提取的四个句子中，三个被改写并用作正确信息。最后一个被改写、否定，然后再次改写，并用作错误信息。释义和否定模型与ReadingQuizMaker系统中使用的模型相同。

按照这个流程，我们为每个部分生成了1-2个多项选择题，总共为整个阅读生成了7-10个问题。自动生成的问题然后通过ReadingQuizMaker界面显示，使用户可以在不喜欢这些问题时进行编辑，如图8所示。

5.3 流程
在研究会话之前，参与者被要求向我们发送他们想要使用的阅读材料。唯一的要求是阅读材料必须具有HTML来源。所有参与者都经历了ReadingQuizMaker条件和基准条件。参与者总是首先进行ReadingQuizMaker条件，因为我们不希望自动生成的问题影响教师自己的设计。在会话期间，我们首先获得参与者的同意，然后演示如何使用ReadingQuizMaker系统。参与者然后有45分钟使用ReadingQuizMaker（任务1）的时间，以及20分钟回顾和可能编辑自动生成的问题（任务2），如图8所示。参与者被要求始终分享其屏幕并大声思考。在每个任务结束时，研究人员会提出后续问题。

5.3.1 任务1：使用ReadingQuizMaker系统创建阅读问题
参与者被要求想象他们在课堂上分配这篇阅读材料并设计测验问题来帮助学生阅读。我们强调质量比数量更重要，因此他们无需追求一定数量的问题。我们还鼓励参与者设计能够引导学生进行高阶思考的问题，以便这些问题可以成为阅读指南。我们明确建议他们不必设计简单的问题来测试学生是否已经阅读了文本。我们还向参与者明确表示，由于我们在初步研究中观察到教师在问题创建方面具有不同的熟练度，因此他们无需追求一定数量的问题。这里的目标是探究ReadingQuizMaker是否能帮助教师设计具有高教育价值的发人深省的问题，而不仅仅是简单的事实性问题。在第一个任务结束时，参与者被要求分享他们的经验。我们明确要求他们评论问题的质量，是否满意，人工智能建议的质量以及他们在过程中遇到的挑战。

5.3.2 任务2：回顾和编辑自动生成的问题
参与者在任务1中使用的阅读材料上获得了7-10个自动生成的问题。参与者被要求阅读问题，分享他们的意见，根据需要进行编辑，或者直接放弃质量较低的问题。参与者使用了类似于ReadingQuizMaker的界面，如图8所示。选项的源句在“Paper Panel”中被突出显示。在任务2结束时，我们要求参与者分享他们对自动生成的问题的看法，并比较他们在使用ReadingQuizMaker（人工智能协作方法）与自动方法时的经验。

5.4 数据分析方法

5.4.1 系统日志分析。系统记录了发送到后端的每个NLP模型的API调用日志。两名研究人员观看了用户研究录像，为每个发送的API调用标记用户是否阅读或使用了建议，以及用户基于AI建议做出了什么修改。如果用户将AI建议作为选项发送到问题创作面板而不使用原始句子，则标记AI建议被采纳。如果用户在界面上停顿并阅读结果，则标记AI建议已阅读。由于有时参与者直接发送原始文本，而不等待AI建议显示。系统还记录了用户是否通过"Immediate Preview"功能或NLP Toolbox采纳了AI建议，后者需要明确的点击。
类似地，我们记录了用户对问题干扰建议的采纳情况。我们记录了用户是否查看了建议以及是否采纳了建议，以及对其进行的修改。如果用户向下滚动问题创作面板以查看更多选项，则标记为已检查。如果用户从菜单中选择了一个干扰建议，则标记为已采纳。用户手写的问题干扰也被记录。

5.4.2 沉思式记录亲和图。录音被转录并使用亲和图[61]进行分析。两名作者解释了记录，迭代地对解释性注释进行分组，并从数据中确定出现的主题。

6 发现
我们针对每个研究问题提出了相应的发现。

6.1 RQ1：所有参与者成功使用ReadingQuizMaker创建了令其满意的问题

所有用户都成功使用ReadingQuizMaker创建了问题。总共创建了89个问题，包括51个多项选择题，28个多项响应题和10个开放式问题。79个多项选择/响应问题总共包含288个选项。每个问题平均包含3.6455个选项。我们要强调的是，在研究之前，我们明确告诉参与者不要关注数量。在研究会话期间，多名参与者提到他们看到了一些简单的问题机会，但这些问题没有针对更高层次的思考，因此他们没有记录下来。

6.1.1 教师对问题质量感到满意。所有参与者都对他们创建的问题质量感到满意，并表示他们会在课堂上使用这些问题，表达了他们的兴奋之情。例如，P12说：“我肯定会在我的课堂上分配这些问题，因为这是[努力]，你知道，重新阅读文章。并且在程序的帮助下，我认为这些问题相当扎实，可以开始对话。” P4说：“我实际上认为它们已经足够好了。除了，就是我想重新排列一些选项的顺序。”教师还表示这个工具帮助他们创建了更有意义的问题（P6）。P10还说：“对我来说，这是有帮助的，因为它让我考虑作为问题作者，什么是一个好的多项选择问题，什么是一个好的提示性问题？...如果我坐在那里不知道该怎么办，那么它就让我，好吧，否定这个或者做一些不同的事情。” P5说：“我认为我使用这个工具制作的关于子目标的问题可能比我随手创造的更好，因为总结给我提供了三个相当简洁的句子关于子目标。”参与者在会话期间创建的示例问题显示在图9中。参与者创建了带有图形的问题、开放式问题、多项选择问题和多项响应问题，引导学生深入思考文本背后的内容。

6.1.2 问题创建被认为更容易更快捷。大多数参与者发现与他们通常的教学设计流程相比，ReadingQuizMaker节省了时间。P13说：“我在阅读完论文后可以轻松地创建问题，我几乎在完成问题和答案。所以这绝对会成为我的时间节省者。” P12说：“我确实认为这在时间上是有帮助的，因为它更快速，并且我认为界面相当简单。”参与者还提到阅读或重新阅读内容是耗时的。多名用户表示如果他们在会话之前阅读了文本会更好，因为他们通常会对他们将分配的内容有更高的熟悉度，从而更快地创建问题。

几乎所有参与者都提到界面易于使用。P2发现整个过程流畅且易于follow，而且问题审查面板对他们查看整体情况很有帮助。P9发现文本选择和添加选项的流程“相当清晰”。P3说：“这是一个新工具，花些时间来熟悉，你知道，所有的基本事情，但这很快。” P8说：“我觉得有了这篇论文和其他支持，真的帮助了我，让我在创建这些问题时有了一个基础。这也使我更容易这样做。通常，我以前不太喜欢创建问题。”参与者发现系统被设计为服务于他们的自然流程并且很灵活。P10说：“你有点看到我的过程，我找到了一些东西，然后把它当作一个答案，然后试图从中向后构建问题。我认为系统相当灵活，能够做到这一点。”

6.2 RQ2：教师认为AI的建议是有用且令人满意的。

首先，我们对教师如何使用ReadingQuizMaker提供的题干建议和AI建议进行了系统日志数据分析。在创建的89个问题中，用户共查看了52次问题题干菜单，并从菜单中选择了41次。用户对推荐的问题题干表示赞赏。P3说：“我看到了推荐，然后我想，哦，为什么我不能使用那些推荐的问题并在这些基础上构建呢？” P8认为问题库给了他“一个开始的灵感”，P10发现它“很好地激发了我，而无需自己创造”。

6.2.1 用户采纳了60%的AI建议。
总结。在所有13个研究会话中，总共触发了37次摘要模型，并由用户检查了37次。这意味着每次触发摘要模型时，用户都会检查结果。我们发现37次建议中有19次被采纳为选项，其中10次被直接使用，9次被拆分为多个选项。摘要模型触发的次数不太多，因为用户需要选择整个段落或多个段落。在研究中，我们观察到用户主要选择句子，对于这些情况，默认建议是释义。

释义。在所有用户研究会话中，释义模型触发了197次。其中只有6次是通过NLP工具箱触发的，其余的是通过AI建议的Paper Panel预览触发的，如图2所示。我们认为在用户选择后预览AI建议是成功的，因为它帮助用户发现了AI支持。191次释义建议中有143次被用户检查，占被检查的建议的75%，88次被接受并采纳到问题中，占被检查的建议的59%。对于被采纳的释义建议，用户进一步编辑了其中的23次。

否定。在所有研究会话中，用户应用并检查了49次否定操作，其中29次被使用，占触发的次数的59%。大多数否定是通过NLP工具箱手动触发的，只有2次是在用户发送新选项时自动触发的。教师使用了需要关键字输入的可控版本9次，其中5次被接受。4次拒绝中的3次是在相同的句子上完成的，P8想要否定句子中的特定单词，但结果并不如预期。P8最终自己否定了它。用户应用否定操作后，其中一些人对选项进行了进一步的编辑。其中有11个选项进行了重大编辑。

6.2.2 教师认为AI的建议是有用且激发灵感的。
在完成任务1后，我们要求参与者评论AI建议的质量。几乎所有参与者都评论说AI给予了他们有用的建议，并帮助他们更容易地创建问题。参与者还提到他们会在使用AI建议之前先阅读AI的结果。例如，P10说：“我喜欢大摘要和分割。我认为这很好，尤其是这篇文章的写作方式，你可以从中选取一段或两段，然后得到很多简单的问题草稿，所以我可以看到自己使用它。” P9发现AI的结果相当令人满意，尤其是在输入较长的情况下：“总的来说，我对AI的满意度还是相当高的。” 特别是P12喜欢释义建议“有时候我实际上对AI的很多释义都印象深刻。嗯，所以我可能会先阅读它。”

我们还要求参与者具体评论AI建议是否分散注意力或妨碍他们自然思考和创建问题。所有参与者都分享说AI建议一点也不分散他们的注意力，而且他们有足够的控制权来决定何时以及如何使用这些建议。P5说：“我是说，这并不是真的分散注意力，因为，就像，我必须通过点击这个transform菜单来拉它上来，然后做这些事情。” 参与者还分享说他们有点担心过多依赖AI，因此他们在采纳之前需要仔细校对AI建议。P6说：“分散注意力吗？不是，尽管我可以看到自己依赖它而犯错误，因为至少有几次我觉得它删除了一个改变含义的短语。”

6.2.3 教师在结果不令人满意时进一步修改AI建议。教师还分享说有时AI的结果并不令人满意。AI可能生成需要进一步修改才能使用的半成品草稿。例如，P11说：“有时候效果很好，有时候不像。有时候它给我一个不错的半成品，我可以利用它，所以我不认为我会直接使用它。” 参与者还表示，即使他们发现结果不令人满意，AI的建议也给了他们灵感，帮助他们进行替代思考。例如，P4根据否定的结果创建了自己的干扰项。P6说：“我敢肯定AI也有一套固定的东西，但它们是一组不同的东西，而不是我拥有的那一组。所以提供这些替代版本真的很好。”

6.2.4 发现AI的可发现性对采纳至关重要。在研究中，我们发现当AI建议能够轻松获得且无需用户额外操作时，用户更有可能检查和采纳这些建议。例如，一旦用户选择了一个句子，释义建议就会自动显示。例如，P2选择了一个句子，希望替换代词。在看到释义建议后，他们选择使用了它。同样，P4起初没有打算释义，并且一开始没有等释义建议加载，但随着他们在选择后看到更多释义建议，他们发现释义是令人满意的，于是选择了它们。在研究后期，P4更有意识地等待释义结果加载或使用NLP工具箱应用释义操作。然而，另一方面，摘要和否定操作需要用户采取额外的步骤才能应用，因此受到的关注较少。特别是对于摘要而言，用户需要选择多个段落。P12提到在研究中途她忘记了如何应用摘要操作。

6.3 RQ3：教师更喜欢ReadingQuizMaker提供的人机协作方法

在研究结束时，我们要求参与者比较他们使用ReadingQuizMaker的经验，该系统采用人机协作方法与自动方法相比。13名教师中有12人更喜欢ReadingQuizMaker提供的人机协作方法。

6.3.1 控制很重要。参与者表示他们喜欢人机协作方法，因为他们觉得更加有控制权。例如，P11说：“我会再次更喜欢第一个任务而不是这个，因为我可以控制生成的内容。” P5提到：“看起来我要自己做更多事情，但实际上，这符合我的期望。” 用户想要掌控不仅是为了使问题与他们的目标保持一致，还为了避免错误。正如P1所说：“如果生成的问题不准确，那我绝对不会使用它。因为没有我自己再次审查它。因为你知道，在教育中，你在课堂上犯的任何错误都会降低你的权威。”

6.3.2 查看和编辑自动生成的问题会限制我的创造力。尽管编辑一系列预先制定的问题可能看起来会减少教师的工作，但用户认为在看到自动生成的问题后，他们的想象力和创造力受到了限制。P5说：“我觉得这会束缚我的创造力，但就像我看到的是一条容易走的路线，就像容易的路线就是把这些问题保持在记住、理解的水平上。” 同样，P8提到：“这是一种权衡，就像，你知道，如果你给我看一些东西，那么我的思考就会受到限制...所以基本上对于我来说，你在那里有一个天花板。如果我从头开始创建，我可以尝试创建更好的。”

6.3.3 自动生成的问题质量较低。用户发现与他们通过ReadingQuizMaker系统创建的问题相比，自动生成的问题质量较低。一个显著的缺陷是自动生成的问题中的选项不是自包含的，而是脱离上下文的。P4说：“我觉得这些句子是从不同的部分挑选出来的，但当你把它们组合在一起并脱离上下文时，真的很难判断。” 参与者还表达了担忧，即即使选项是准确的，它们也不是良好的问题选项，因为它们没有针对他们的教学目标。P10说：“我再次会看这个，并说，对于其中的每一个，它代表了其中的文本。但作为一个问题答案，它们能够独立存在吗？” 类似的，P9说：“我不会包含这样的问题。那里没有任何学习的内容，这只是一个故事。” P8评论说：“这两个不相关。它们是背景，但我不会把它们包含在问题中，因为我不需要学生了解背景。” 总的来说，研究中揭示的自动化方法的主要缺点是，尽管它能够生成逻辑上合理的问题和选项，但这些选项可能脱离上下文，不符合教师的教育目标。

6.3.4 如果我没有时间，有些问题还是不错的。尽管大多数参与者更喜欢通过ReadingQuizMaker界面自己创建问题，但其中一些人发现如果时间有限，自动生成的问题是可以接受的。例如，P3说：“我认为第一个问题很棒。而且选项实际上是非常有分析意义的。” 一些参与者发现自动生成的问题与他们在任务1中创建的问题之间存在相似之处。P8提到：“这仍然相当不错。我认为我创建的问题也差不多，就像，我有这个选项，我有这个选项，但更详细。” 用户承认，如果他们没有足够的时间，他们可能愿意使用这些自动生成的问题。

6.4 RQ4: 用户在ReadingQuizMaker中的挑战和体验以及设计启示

尽管参与者普遍认为ReadingQuizMaker系统易于使用并帮助他们创建更高质量的问题，但他们在这个过程中报告了一些挑战。用户通常发现想出迷惑项是具有挑战性的。P5说：“我认为制作迷惑项对于单项选择题来说是最难的部分，无论是对于模型还是对于人类。” 的确，许多教师在创建迷惑项时都遇到了困难。例如，P10上下滚动，花了几分钟浏览各个部分，以找到一个迷惑项。参与者面临的另一个挑战是找到合适的问题干。在研究中，我们看到问题干建议得到了参与者的充分利用。然而，问题干库主要专注于学术论文。当参与者为在线教程、教科书章节和新闻文章设计测验问题时，他们并没有发现问题干建议很有用。P8有一个问题要问，但在措辞问题干时感到很困难。同样，P5说“知道我想如何措辞问题”是有挑战的。但他发现问题干建议很有帮助，表示“很酷，我得到了自动补全来给我提供一些想法”。

7 使用案例演示
我们演示了两个场景，学生可以使用ReadingQuizMaker系统生成的问题进行学习。

7.1 通过现有学习管理平台用作形成或总结性评估
在ReadingQuizMaker中创建的问题可以下载为.csv文件，并转换为.QTI包，可以直接导入到现有的学习管理系统中，如Canvas。这使得教师能够将他们在系统中创建的问题作为形成或总结性评估分配给学生。

7.2 用作学生的阅读指南
在ReadingQuizMaker中创建的问题也可以制作成一个互动的阅读指南。学生可以通过支持问题回答的扩展ReadingQuizMaker界面回答生成的问题。如果他们答错了一个问题，界面会提供反馈，突出显示内容在阅读中的位置，这在问题创建过程中已经记录了。

8 讨论
在本节中，我们讨论了潜在的未来方向。在评估研究中，我们发现参与者对ReadingQuizMaker系统的反馈非常好。教师发现系统易于使用，直观，问题干建议和AI建议有用，并且他们能够在系统的支持下创建高质量的问题。教师还表现出对ReadingQuizMaker提供的人机协作方法比自动方法更强烈的偏好。在这里，我们讨论了教师问题创建过程中的一些挑战，并提出了未来的方向来解决这些挑战。

8.1 提高AI输出的发现性、可视化和可解释性
在研究中，我们发现用户更愿意检查和采纳AI建议，当它们能够轻松获得并且不需要用户额外的操作时。例如，与总结和否定操作相比，用户更频繁地检查和采纳由用户选择句子后自动显示的释义建议。这符合人机交互指南，支持对AI结果进行高效的调用、撤销和更正。此外，我们发现一些用户发现难以解析自然语言处理（NLP）的结果，尤其是总结。用户需要阅读原始段落并阅读摘要以确保准确性。在ReadingQuizMaker中，我们实现了可视化，以突出显示实体替换和否定的变化。然而，我们没有为总结和释义模型提供可视化，因为其中一些涉及较大的更改。未来的工作需要研究更好的可视化技术，以帮助用户更有效地感知NLP的结果。最后，一些用户发现很难理解模型是如何生成结果的。这尤其适用于否定模型。一些用户想知道为什么模型选择否定某个特定的单词，而其他人更喜欢我们提供的用户可控版本的否定模型。未来的工作需要探索向最终用户解释生成模型的技术，并为用户提供控制NLP模型的方法。

8.2 支持教育内容创建的人工智能协作方法
我们的研究参与了先前工作，展示了人工智能协作方法在支持教育方面的威力[41, 42, 83]。我们发现在问题创建中，用户更倾向于人工智能与人类协作而非自动方法。在ReadingQuizMaker中，用户完全掌握了整个过程。这不仅让用户感到安全，还使整个过程更加流畅。一些用户表示他们想要跟踪整个过程，例如，他们使用了哪些文本，每个选项来自阅读的哪个部分等。另一方面，用户发现自动生成的问题质量较低。其中一个原因是，对于AI来说很难确定要关注的内容，即为何创建问题的重要内容。当用于AI模型的输入与用户的期望不符时，释义或否定的句子将失去上下文，并且作为问题选项时不够令人满意。自动化方法可能会生成逻辑上正确的多项选择问题，但教师发现选项脱离上下文且毫无意义。我们认为，在需要专业知识和创造力的教育内容创作中，人类参与以指定AI系统输入是必要的，并有助于提高对AI建议的采纳率。随着大型语言模型（LLM）如ChatGPT越来越普及，研究人员一直在探索如何利用LLM来达到教育目标，我们的结果提供了重要建议：对于高风险的任务，如教育内容创作，允许用户提供输入并充分掌控整个过程比完全自动化的方法更可取。

另一个方向是使用可控的AI模型，用户可以指定额外的参数。我们为否定操作实现了一个简单的可控版本，结果是可以接受的。然而，有时否定的结果并不令人满意。例如，用户可能选择要否定的特定单词，而模型选择了相邻的单词进行否定。未来的工作需要开发和整合可控的自然语言处理模型，并帮助用户指定参数并获得符合其目标的结果。对于教育内容创作，包括“焦点”词对于否定和释义模型都可以减少丢失重要信息的风险，并生成符合用户期望的结果。

8.3 改进问题创建系统的启示
在创建问题时最大的挑战之一是知道该问什么。一些参与者发现ReadingQuizMaker提供的摘要具有启发性，能够给予他们创意，但是知道要摘要什么以及如何将摘要结果转化为问题仍然是具有挑战性的。未来的工作可以探索帮助人们识别问题机会的方法。许多用户发现很难想出好的干扰项。尽管许多参与者使用否定来生成干扰项，但想出一个合理的错误选项仍然是具有挑战性的。基于人们展示的策略，我们建议未来的工作探索从文本的特定位置提取信息，例如相关工作部分，以生成干扰项。在当前版本的ReadingQuizMaker中，问题茎主要设计用于学术论文，并未专门针对教程和教科书等其他阅读文本进行定制。随着ReadingQuizMaker用户数量的增加和多样化，众包的问题库将不断增长，并在未来为用户提供更多多样化的建议。在ReadingQuizMaker中，用户需要等待2-3秒，以加载AI建议。在评估研究中，我们没有观察到用户因延迟而感到烦恼或分心，因为他们通常会利用这段时间阅读原文。未来将大型语言模型整合到用户界面的工作需要减少延迟，并检查对用户体验的潜在影响。


9 限制

1）参与者样本较小。未来，我们计划进行更大规模的研究，以了解教师如何使用类似ReadingQuizMaker的工具来创建问题。还需要进行纵向研究，以了解教师如何与系统建立信任，以及他们是否随着时间的推移在使用系统方面变得更加熟练。
2) 在评估研究中，我们要求用户自行报告该方法是否节省了他们创建问题的时间。这旨在调查教师在实践中采用类似ReadingQuizMaker系统的可能性。需要进行更全面的定量研究，以充分调查系统的节省时间因素。
3) 本研究专注于面向教师的评估研究，目标是从教师的角度调查ReadingQuizMaker的可用性和效用。需要进行大规模的面向学生的实验，以了解生成的问题是否有助于学生的阅读理解和学习。

10 结论
我们提出了ReadingQuizMaker，支持教师便利地设计高质量问题，帮助学生理解阅读材料。ReadingQuizMaker适应教师创建问题的自然工作流程，同时提供基于自然语言处理的过程性支持。ReadingQuizMaker使教师能够决定何时以及使用哪些自然语言处理模型，选择模型的输入，并编辑结果。在评估研究中，教师发现生成的问题与他们之前设计的测验相媲美。教师称赞ReadingQuizMaker易于使用，并认为自然语言处理的建议令人满意且有帮助。我们将ReadingQuizMaker与另一条件进行了比较，该条件下教师被提供自动生成的问题进行编辑。教师明显更喜欢由ReadingQuizMaker提供的人工智能协作方法。与ReadingQuizMaker相比，教师发现自动生成的问题质量较低，问题中的内容脱离上下文且毫无意义。我们的发现为使用大型语言模型支持教育提供了重要建议。我们认为，在高风险任务，如教育内容创建中，允许用户提供输入并在过程中给予用户足够的控制更可取，而不是完全自动化的方法。


